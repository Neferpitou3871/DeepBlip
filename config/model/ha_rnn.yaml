# @package _global_
model:
  hidden_size: 50
  num_layer: 1
  hr_size: 30
  dropout_rate: 0.1
  fc_hidden_size: 15

  optimizer:
    optimizer_cls: 'adam'
    learning_rate: 0.001
    weight_decay: 0.0
    lr_scheduler: False
    #scheduler
    lr_scheduler_cls: 'ExponentialLR'
    gamma: 0.99

checkpoint:
  monitor: 'val_loss'

exp:
  max_epochs: 15                   # Number of epochs
  batch_size: 2                   # Batch size
  load_pretrained: False           # Load pretrained model
  resume_train: False             # Resume training
  run_id: 0                       # Run ID of the checkpoint
  exp_id: 0                        # Experiment ID of the checkpoint